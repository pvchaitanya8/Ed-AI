import streamlit as st
import speech_recognition as sr
from audio_recorder_streamlit import audio_recorder
import os
import tempfile
from gtts import gTTS
from io import BytesIO
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import SystemMessage, HumanMessage, AIMessage
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Function for capturing and transcribing audio
def capture_audio():
    recognizer = sr.Recognizer()
    audio_bytes = audio_recorder()
    
    if audio_bytes:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_audio_file:
            audio_file_path = temp_audio_file.name
            temp_audio_file.write(audio_bytes)
        with sr.AudioFile(audio_file_path) as source:
            audio_data = recognizer.record(source)
            try:
                transcription = recognizer.recognize_google(audio_data)
                return transcription
            except sr.UnknownValueError:
                st.write("Could not understand the audio.")
            except sr.RequestError as e:
                st.write(f"Error with the transcription service: {e}")
        os.remove(audio_file_path)
    return None

# Function for text-to-speech conversion
def text_to_speech(text):
    tts = gTTS(text=text, lang='en')
    audio_buffer = BytesIO()
    tts.write_to_fp(audio_buffer)
    audio_buffer.seek(0)
    return audio_buffer

# Initialize session state for chat history
def initialize_session_state():
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

# Initialize the language model
def initialize_llm():
    return ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.7)

# Build chat messages for the LLM
def build_messages(history, user_input, system_prompt):
    messages = [SystemMessage(content=system_prompt)]
    for chat in history:
        messages.append(HumanMessage(content=chat['user_input']))
        messages.append(AIMessage(content=chat['response']))
    messages.append(HumanMessage(content=user_input))
    return messages

# Handle user input and generate AI response
def handle_query(query, llm, system_prompt):
    messages = build_messages(st.session_state.chat_history, query, system_prompt)
    response_message = llm(messages)
    
    if isinstance(response_message, AIMessage):
        response = response_message.content.strip()
    else:
        response = "I'm sorry, I couldn't process your request."
    
    st.session_state.chat_history.append({"user_input": query, "response": response})
    return response

# Display the mock interview page
def Mock_Interview():
    # Initialize session state
    initialize_session_state()

    # Custom CSS for centering all content
    st.markdown("""
    <style>
    .centered-content {
        display: flex;
        justify-content: center;
        align-items: center;
        flex-direction: column;
        text-align: center;
    }
    .mentor-title {
        font-size: 28px;
        font-weight: bold;
        background: linear-gradient(90deg, #FFF5EE, #F3CFC6, #f9bec7, #ffafcc, #f72585, #b5179e, #7209b7, #560bad, #480ca8, #3a0ca3, #3f37c9, #4361ee, #4895ef, #4cc9f0, #caf0f8, #FFF5EE, #FFF5EE);
        background-clip: text;
        -webkit-background-clip: text;
        color: transparent;
        background-size: 200% 200%;
        animation: gradientFlow 5s ease infinite;
        margin-top: 0;
        margin-bottom: 15px;
    }
    @keyframes gradientFlow {
        0% { background-position: 0% 50%; }
        50% { background-position: 100% 50%; }
        100% { background-position: 0% 50%; }
    }
    </style>
    <div class="centered-content">
        <h1 class="mentor-title">âœ¨ Mock Interview</h1>
    </div>
    """, unsafe_allow_html=True)

    # Center the audio recording and response content
    st.markdown('<div class="centered-content">', unsafe_allow_html=True)
    user_input = capture_audio()

    if user_input:
        llm = initialize_llm()

        system_prompt = (
            "You are an AI conducting a professional interview. Ask questions based on the user's responses, "
            "follow up with clarifications, and provide feedback on their answers."
        )
        response = handle_query(user_input, llm, system_prompt)

        # Display AI's audio response and text
        audio_response = text_to_speech(response)
        st.audio(audio_response, format="audio/mp3")

    
    else:
        response = None
    
    # Ensure response and user_input are not None before rendering
    if user_input:
        st.markdown(f'<p style="text-align:right; margin-bottom:10px; color:gray;"><strong>You:</strong> {user_input}</p>', unsafe_allow_html=True)
    if response:
        st.markdown(f'<p style="text-align:left; margin-bottom:5px;"><strong>Interviewer:</strong> {response}</p>', unsafe_allow_html=True)

    st.markdown('</div>', unsafe_allow_html=True)

if __name__ == "__main__":
    Mock_Interview()
